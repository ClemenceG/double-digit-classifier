{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-27T03:06:02.134841Z","iopub.execute_input":"2022-10-27T03:06:02.135332Z","iopub.status.idle":"2022-10-27T03:06:02.148196Z","shell.execute_reply.started":"2022-10-27T03:06:02.135282Z","shell.execute_reply":"2022-10-27T03:06:02.146497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Model","metadata":{}},{"cell_type":"code","source":"# # y_hat = sigmoid(w.X + b)\n# # from tqdm import tqdm\n\n\n# logistic = lambda z: 1./ (1 + np.exp(-z))\n# class LogisticRegression:  \n#   \"\"\"\n#   M: Number of classes\n#   D: Feature Dimension\n#   N: Number of rows\n#   \"\"\"\n#   def __init__(self, learning_rate=.001, max_iters=1e4, eps=1e-8):\n#     self.learning_rate = learning_rate\n#     self.max_iters = max_iters\n#     self.epsilon = eps\n    \n\n#   def gradient(self, x, y, params):\n#     # assumes x has a bias term (ones)\n#     N = x.shape[0] # N number of rows\n#     w = params\n\n#     yh = logistic(np.dot(x, w))\n#     dy = yh - y\n#     dw = np.dot(x.T, dy)/N \n#     dparams = dw\n#     return dparams\n\n#   def error(self, x, y, params):\n#     y_probs = logistic(np.matmul(x, params))\n#     yh = np.argmax(y_probs, axis=1)\n#     return (yh != y).sum()/x.shape[0]\n\n#   def fit(self, x, y, k=10):\n#     N = x.shape[0]\n#     x = np.c_[ x, np.ones(N) ]\n#     D = x.shape[1]\n\n#     self.M = len(np.unique(y)) # Number of classes\n\n#     train_idx, valid_idx = [], []\n\n#     # Create Validation and Train sets\n#     indices = np.arange(x.shape[0])\n#     splits = np.array_split(indices, k)\n\n#     valid_idx = splits[0].tolist()\n#     temp_train = []\n#     for i in range(1, len(splits)-1):\n#       temp_train += splits[i].tolist()\n\n#     x_train, y_train = x[temp_train, :], y[temp_train]\n#     x_valid, y_valid = x[valid_idx, :], y[valid_idx]\n\n#     # Encode Y\n#     y_train_enc = pd.get_dummies(y_train)\n\n#     # Init parameters\n#     params = np.random.randn(D, self.M) * .01\n#     self.params = params\n\n#     # Gradient Descent\n#     grads_normed = np.array([np.inf]*3)\n#     valid_err = [np.inf]\n#     # print(self.error(x_valid, y_valid, params))\n#     t = 1\n# #     pbar = tqdm(total=self.max_iters)\n#     while (np.any(grads_normed > self.epsilon) and t < self.max_iters\n#         if valid_err[-1] < (self.error(x_valid, y_valid, params)): break\n#         self.params = params\n#         t += 1\n#         valid_err.append(self.error(x_valid, y_valid, params))\n\n#         grad = self.gradient(x_train, y_train_enc, params)\n#         params -= self.learning_rate * grad\n\n#         grads_normed = np.array([np.linalg.norm(g) for g in grad])\n# #         pbar.update(1)\n# #     pbar.close()\n# #     print(valid_err)\n#     # self.params = self.grad_descent(self.gradient, x_train, y_train_enc, w)\n\n\n#   def predict(self, x):\n#       w = self.params\n#       x = np.c_[ x, np.ones(x.shape[0]) ]\n#       y_probs = logistic(np.dot(x, w)) \n#       yh = np.argmax(y_probs, axis=1)\n#       yh = np.c_[np.arange(0, yh.shape[0], 1, dtype=int), yh]\n#       return yh","metadata":{"execution":{"iopub.status.busy":"2022-10-25T23:09:58.267453Z","iopub.execute_input":"2022-10-25T23:09:58.267851Z","iopub.status.idle":"2022-10-25T23:09:58.308296Z","shell.execute_reply.started":"2022-10-25T23:09:58.267773Z","shell.execute_reply":"2022-10-25T23:09:58.306749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pull Data","metadata":{}},{"cell_type":"code","source":"x_train = np.genfromtxt(\"/kaggle/input/classification-of-mnist-digits/train.csv\", delimiter=',', skip_header=1)\nx_train = x_train[:, :-1]\n# print(x_train.shape)\n\nlabels = np.genfromtxt('/kaggle/input/classification-of-mnist-digits/train_result.csv', delimiter=',', skip_header=1)\nlabels = labels[:, 1]\n# print(labels.shape)\n\nx_test = np.genfromtxt('/kaggle/input/classification-of-mnist-digits/test.csv', delimiter=',', skip_header=1)\nx_test = x_test[:, :-1]","metadata":{"execution":{"iopub.status.busy":"2022-10-27T03:27:40.245865Z","iopub.execute_input":"2022-10-27T03:27:40.249142Z","iopub.status.idle":"2022-10-27T03:30:12.249058Z","shell.execute_reply.started":"2022-10-27T03:27:40.249076Z","shell.execute_reply":"2022-10-27T03:30:12.247745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run Logistic\n","metadata":{}},{"cell_type":"code","source":"# log_mod = LogisticRegression()\n# log_mod.fit(x_train, labels)\n# yh_test = log_mod.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-10-24T15:18:21.898435Z","iopub.execute_input":"2022-10-24T15:18:21.898836Z","iopub.status.idle":"2022-10-24T15:19:17.442575Z","shell.execute_reply.started":"2022-10-24T15:18:21.898804Z","shell.execute_reply":"2022-10-24T15:19:17.440817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LeNet","metadata":{}},{"cell_type":"code","source":"def split_train_valid(x, y, k=5):\n  # Create Validation and Train sets\n    indices = np.arange(x.shape[0])\n    splits = np.array_split(indices, k)\n\n    valid_idx = splits[0].tolist()\n    temp_train = []\n    for i in range(1, len(splits)-1):\n      temp_train += splits[i].tolist()\n    \n    x_train = x[temp_train, :] \n    y_train = y.iloc[temp_train, :]\n    x_valid, y_valid = x[valid_idx, :], y.iloc[valid_idx, :]\n\n    return x_train, y_train, x_valid, y_valid","metadata":{"execution":{"iopub.status.busy":"2022-10-27T03:08:53.977312Z","iopub.execute_input":"2022-10-27T03:08:53.977808Z","iopub.status.idle":"2022-10-27T03:08:53.985669Z","shell.execute_reply.started":"2022-10-27T03:08:53.977763Z","shell.execute_reply":"2022-10-27T03:08:53.984615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\nfrom tensorflow.keras.optimizers import SGD\n# metrics \nfrom keras.metrics import categorical_crossentropy\n\ndef LeNet():\n  model = Sequential()\n  \n  # Convolutional layer  \n  model.add(Conv2D(filters = 6, kernel_size = (5,5), padding = 'same', \n                   activation = 'relu', input_shape = (28,28,1)))\n  \n  # Max-pooing layer with pooling window size is 2x2\n  model.add(MaxPooling2D(pool_size = (2,2)))\n  \n  # Convolutional layer \n  model.add(Conv2D(filters = 16, kernel_size = (5,5), activation = 'relu'))\n  \n  # Max-pooling layer \n  model.add(MaxPooling2D(pool_size = (2,2)))\n  \n  # Flatten layer \n  model.add(Flatten())\n  \n  # The first fully connected layer \n  model.add(Dense(120, activation = 'relu'))\n  \n  # The output layer  \n  model.add(Dense(10, activation = 'softmax'))\n  \n  # compile the model with a loss function, a metric and an optimizer function\n  # In this case, the loss function is categorical crossentropy, \n  # we use Stochastic Gradient Descent (SGD) method with learning rate lr = 0.01 to optimize the loss function\n  # metric: accuracy \n  \n  opt = SGD(lr = 0.01)\n  model.compile(loss = categorical_crossentropy, \n                optimizer = opt, \n                metrics = ['accuracy']) \n                \n  return model","metadata":{"execution":{"iopub.status.busy":"2022-10-27T03:10:23.766743Z","iopub.execute_input":"2022-10-27T03:10:23.767713Z","iopub.status.idle":"2022-10-27T03:10:23.778706Z","shell.execute_reply.started":"2022-10-27T03:10:23.767665Z","shell.execute_reply":"2022-10-27T03:10:23.777012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_lenet(model, x, y, batch_size=128, epochs = 50):\n    '''\n    x (_, 28, 28, 1)\n    y one hot encoded : (_, 10)\n\n    '''\n\n    # Split Train/Validation\n    x_train, y_train, x_valid, y_valid = split_train_valid(x, y)\n    # batch_size = 128\n    # print(y_valid0.shape)\n    # print(y_train0.shape)\n\n    acc = model.fit(x_train, y_train, epochs = epochs,       # HYPERPARAMETER \n                            batch_size = batch_size,             # HYPERPARAMETER\n                          steps_per_epoch = x_train.shape[0]//batch_size, \n                          validation_data = (x_valid, y_valid), \n                          validation_steps = x_valid.shape[0]//batch_size, verbose = 1)\n    return None\n\ndef predict(model, x):\n  '''\n  input:\n  - x: (_, 28, 28, 1)\n  output:\n  - y: (_, 1)\n  '''\n  vec_p = model.predict(x)\n  # determine the label corresponding to vector vec_p\n  y_p = np.argmax(vec_p, axis=1)\n#   print(vec_p)\n#   print(y_p)\n  return y_p","metadata":{"execution":{"iopub.status.busy":"2022-10-27T03:12:16.833451Z","iopub.execute_input":"2022-10-27T03:12:16.833886Z","iopub.status.idle":"2022-10-27T03:12:16.843804Z","shell.execute_reply.started":"2022-10-27T03:12:16.833851Z","shell.execute_reply":"2022-10-27T03:12:16.842328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run LeNet","metadata":{}},{"cell_type":"code","source":"x_train_tens, x_train_ones = x_train[:, :784].reshape(x_train.shape[0], 28, 28, 1), x_train[:, 784:].reshape(x_train.shape[0], 28, 28, 1)\ny_train_tens, y_train_ones = labels//10, labels % 10\nx_test_tens, x_test_ones = x_test[:, :784].reshape(x_test.shape[0], 28, 28, 1), x_test[:, 784:].reshape(x_test.shape[0], 28, 28, 1)\n# print(y_train_tens.shape)\n\nzeros = pd.DataFrame(np.zeros((y_train_tens.shape[0], 8)))\ny_train_tens, y_train_ones = pd.concat([pd.get_dummies(y_train_tens), zeros],axis=1), pd.get_dummies(y_train_ones)","metadata":{"execution":{"iopub.status.busy":"2022-10-27T03:12:19.134797Z","iopub.execute_input":"2022-10-27T03:12:19.135341Z","iopub.status.idle":"2022-10-27T03:12:19.159678Z","shell.execute_reply.started":"2022-10-27T03:12:19.135300Z","shell.execute_reply":"2022-10-27T03:12:19.158397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mod_tens = LeNet()\ntrain_lenet(mod_tens, x_train_tens, y_train_tens)\ny_tens = predict(mod_tens, x_test_tens)","metadata":{"execution":{"iopub.status.busy":"2022-10-27T03:12:20.345061Z","iopub.execute_input":"2022-10-27T03:12:20.346072Z","iopub.status.idle":"2022-10-27T03:12:33.194760Z","shell.execute_reply.started":"2022-10-27T03:12:20.346024Z","shell.execute_reply":"2022-10-27T03:12:33.193676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mod_ones = LeNet()\ntrain_lenet(mod_ones, x_train_ones, y_train_ones)\ny_ones = predict(mod_ones, x_test_ones)","metadata":{"execution":{"iopub.status.busy":"2022-10-27T03:13:30.902856Z","iopub.execute_input":"2022-10-27T03:13:30.903345Z","iopub.status.idle":"2022-10-27T03:15:57.573852Z","shell.execute_reply.started":"2022-10-27T03:13:30.903308Z","shell.execute_reply":"2022-10-27T03:15:57.572609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine y_ones y_tens\ny_pred = np.c_[np.arange(y_tens.shape[0]), y_tens*10+y_ones]\nnp.savetxt(\"/kaggle/working/submission.csv\", y_pred, delimiter=\",\", header=\"Index,Class\", fmt=\"%i\", comments='')","metadata":{"execution":{"iopub.status.busy":"2022-10-27T03:12:51.906676Z","iopub.execute_input":"2022-10-27T03:12:51.908051Z","iopub.status.idle":"2022-10-27T03:12:51.936593Z","shell.execute_reply.started":"2022-10-27T03:12:51.907999Z","shell.execute_reply":"2022-10-27T03:12:51.935012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}